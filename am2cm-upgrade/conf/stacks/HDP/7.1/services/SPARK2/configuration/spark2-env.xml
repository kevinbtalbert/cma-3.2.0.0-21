<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
<configuration supports_adding_forbidden="true">
  <!-- spark-env.sh -->
  <property>
    <name>content</name>
    <description>This is the jinja template for spark-env.sh file</description>
    <value>
#!/usr/bin/env bash

# This file is sourced when running various Spark programs.
# Copy it as spark-env.sh and edit that to configure Spark for your site.

# Options read in YARN client mode
#SPARK_EXECUTOR_INSTANCES="2" #Number of workers to start (Default: 2)
#SPARK_EXECUTOR_CORES="1" #Number of cores for the workers (Default: 1).
#SPARK_EXECUTOR_MEMORY="1G" #Memory per Worker (e.g. 1000M, 2G) (Default: 1G)
#SPARK_DRIVER_MEMORY="512M" #Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)
#SPARK_YARN_APP_NAME="spark" #The name of your application (Default: Spark)
#SPARK_YARN_QUEUE="default" #The hadoop queue to use for allocation requests (Default: default)
#SPARK_YARN_DIST_FILES="" #Comma separated list of files to be distributed with the job.
#SPARK_YARN_DIST_ARCHIVES="" #Comma separated list of archives to be distributed with the job.

{% if security_enabled %}
export SPARK_HISTORY_OPTS='-Dspark.ui.filters=org.apache.hadoop.security.authentication.server.AuthenticationFilter -Dspark.org.apache.hadoop.security.authentication.server.AuthenticationFilter.params="type=kerberos,kerberos.principal={{spnego_principal}},kerberos.keytab={{spnego_keytab}}"'
{% endif %}


# Generic options for the daemons used in the standalone deploy mode

# Alternate conf dir. (Default: ${SPARK_HOME}/conf)
export SPARK_CONF_DIR=${SPARK_CONF_DIR:-{{spark_home}}/conf}

# Where log files are stored.(Default:${SPARK_HOME}/logs)
#export SPARK_LOG_DIR=${SPARK_HOME:-{{spark_home}}}/logs
export SPARK_LOG_DIR={{spark_log_dir}}

# Where the pid file is stored. (Default: /tmp)
export SPARK_PID_DIR={{spark_pid_dir}}

#Memory for Master, Worker and history server (default: 1024MB)
export SPARK_DAEMON_MEMORY={{spark_daemon_memory}}m

# A string representing this instance of spark.(Default: $USER)
SPARK_IDENT_STRING=$USER

# The scheduling priority for daemons. (Default: 0)
SPARK_NICENESS=0

export HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-{{hadoop_conf_dir}}}

# The java implementation to use.
export JAVA_HOME={{java_home}}

# Add Hadoop lzo jar to the classpath (if found)
if [ -z "${HADOOP_VERSION}" ]; then
    if [ `command -v hdp-select` ]; then
        HADOOP_VERSION=`hdp-select status | grep hadoop-client | awk -F " " '{print $3}'`
    else
        echo -e "command hdp-select is not found, please manually set HADOOP_VERSION in spark-env.sh or current environment" 1>&amp;2
        exit 1
    fi
fi

HADOOP_LZO_JAR=
HADOOP_LZO_DIR="/usr/hdp/${HADOOP_VERSION}/hadoop/lib"
num_jars="$(ls -1 "$HADOOP_LZO_DIR" | grep "^hadoop-lzo.*${HADOOP_VERSION}\.jar$" | wc -l)"
if [ "$num_jars" -eq "0" -a -z "$HADOOP_LZO_JAR" ]; then
    HADOOP_LZO_JAR=
elif [ "$num_jars" -gt "1" ]; then
    echo "Found multiple Hadoop lzo jars in $HADOOP_LZO_DIR:" 1>&amp;2
    echo "Please remove all but one jar." 1>&amp;2
    exit 1
elif [ "$num_jars" -eq "1" ]; then
    LZO_JARS="$(ls -1 "$HADOOP_LZO_DIR" | grep "^hadoop-lzo-.*${HADOOP_VERSION}\.jar$" || true)"
    HADOOP_LZO_JAR="${HADOOP_LZO_DIR}/${LZO_JARS}"
fi

export SPARK_DIST_CLASSPATH=${SPARK_DIST_CLASSPATH}:${HADOOP_LZO_JAR}
    </value>
    <value-attributes>
      <type>content</type>
    </value-attributes>
    <on-ambari-upgrade add="true"/>
  </property>
</configuration>
